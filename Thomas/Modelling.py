# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.13.0
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import lightgbm as lgb
from functools import partial
import optuna
import graphviz
from sklearn.metrics import recall_score
df = pd.read_csv('../Dataset/Malware_Classification.csv/Malware_Classification.csv')

features_tree_based = ['Characteristics',
 'DllCharacteristics',
 'SectionsMaxEntropy',
 'MajorSubsystemVersion',
 'Subsystem',
 'ResourcesMaxEntropy',
 'ResourcesMinEntropy',
 'VersionInformationSize',
 'ImageBase',
 'SizeOfOptionalHeader',
 'MajorOperatingSystemVersion',
 'SectionsMinEntropy',
 'SectionsMeanEntropy',
 'CheckSum',
 'SectionsNb',
 'MinorLinkerVersion',
 'ResourcesMeanEntropy',
 'MinorOperatingSystemVersion',
 'SizeOfStackReserve',
 'ResourcesMinSize']

feature_KBest_Based = ['ImageBase',
 'CheckSum',
 'SizeOfStackReserve',
 'SizeOfUninitializedData',
 'LoadConfigurationSize',
 'SizeOfInitializedData',
 'LoaderFlags',
 'BaseOfCode',
 'ResourcesMinSize',
 'AddressOfEntryPoint',
 'SizeOfImage',
 'SizeOfHeapCommit',
 'SectionMaxVirtualsize',
 'SectionsMinVirtualsize',
 'SizeOfCode',
 'BaseOfData',
 'DllCharacteristics',
 'SectionAlignment',
 'SectionsMeanVirtualsize',
 'Characteristics']

y = df['legitimate']
X = df[feature_KBest_Based]

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.3, stratify = df['legitimate'])

print('percentage of stratify split')
print('test')
display((y_test.value_counts()/y_test.value_counts().sum())*100)
print('train')
display((y_train.value_counts()/y_train.value_counts().sum())*100)


lgb_train = lgb.Dataset(X_train, y_train)
lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)
params = {
    'num_leaves': 10,
    'metric': ['auc'],
    'verbose': -1
}
evals_result = {}  # to record eval results for plotting
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=100,
                valid_sets=[lgb_train, lgb_test],
                feature_name=list(X_train.columns),#[f'f{i + 1}' for i in range(X_train.shape[-1])],#[X_train.columns],
                evals_result=evals_result,
                verbose_eval=10)


def render_metric(metric_name):
    ax = lgb.plot_metric(evals_result, metric=metric_name, figsize=(10, 5))
    plt.show()


render_metric(params['metric'][0])


def render_plot_importance(importance_type, max_features=10,
                           ignore_zero=True, precision=3):
    ax = lgb.plot_importance(gbm, importance_type=importance_type,
                             max_num_features=max_features,
                             ignore_zero=ignore_zero, figsize=(12, 8),
                             precision=precision)
    plt.show()


render_plot_importance(importance_type='split')

from sklearn.preprocessing import StandardScaler
ssc = StandardScaler()
ssc.fit_transform(X_train)


# +

def objective(trial):
    # 2. Suggest values of the hyperparameters using a trial object.
    param = {
        'objective': 'binary',
        'metric': 'auc',
        'verbosity': -1,
        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),
        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),
        'num_leaves': trial.suggest_int('num_leaves', 2, 256),
        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),
        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        #'feature_pre_filter':False
    }
    #d_train=lgb.Dataset(X_train, label=y_train)
    #gbm = lgb.train(params, d_train)
    gbm = lgb.LGBMClassifier()
    gbm.set_params(**param) 
    gbm.fit(X_train, y_train)
    
    yhat = gbm.predict(X_test)
    yhat = yhat.round(0)
    recall = recall_score(yhat,y_test)
    return recall

# 3. Create a study object and optimize the objective function.
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=10)
# -

h
