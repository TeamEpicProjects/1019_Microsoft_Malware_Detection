# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.13.0
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# +
import pandas as pd
import numpy as np
import missingno as mso
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import RFE
from sklearn.svm import SVC
from sklearn.decomposition import PCA


df = pd.read_csv('../Dataset/Malware_Classification.csv/Malware_Classification.csv')
df.drop(['ID',"MajorImageVersion","Unnamed: 57"], axis=1, inplace=True)

#X = df.drop(['Machine','legitimate','md5','MajorImageVersion'], axis = 1)
#y = df['legitimate']


# +
def remove_multicoll(CORR, threshold=0.8):
    '''
    removing all features with corr above threshold
    '''
    l = []
    for i in CORR.columns:
        for x,j in enumerate(CORR[i]):
            if j>threshold:
                if i != CORR.columns[x]:
                    l.append((i,CORR.columns[x]))
    for i in l:
        for j in l:
            if i==j:
                l.remove(j)
    
    Remove = []
    keep = []
    for i in l:
        if i[0] not in keep:
            keep.append(i[0])
        if i[1] not in Remove:
            Remove.append(i[1])
    return Remove, keep

R, k = remove_multicoll(df.corr())
# -

df.drop(R, axis=1, inplace=True)


X = df.drop(['Machine','legitimate','md5'], axis = 1)
y = df['legitimate']







# # Working on Feature Selection of the dataset using the following :
# ## 1. Boruta
# ## 2. RFECV
# ## 3. RFE with permutation
# ## 4. SelectKBest
# ## 5. PCA

# ### Starting with RFECV : Recursive Feature Elimination with Cross Validation

svc = SVC(kernel="linear", C=1)
rfe = RFE(estimator=svc, n_features_to_select=1, step=1)
rfe.fit(X, y)
ranking = rfe.ranking_
print(ranking)
plt.matshow(ranking, cmap=plt.cm.Blues)
plt.colorbar()
plt.title("Ranking of pixels with RFE")
plt.show()









# # PCA

pca = PCA(n_components=len(X.columns))
pca.fit(X)
print(pca.explained_variance_ratio_)
print(pca.singular_values_)

# # SelectKBest

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

bestfeatures = SelectKBest(score_func=chi2, k=10)
fit = bestfeatures.fit(X,y)

dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)

#concat two dataframes for better visualization 
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Specs','Score']  #naming the dataframe columns

featureScores.sort_values(by=['Score'], ascending = False)

featureScores.nlargest(10, 'Score')

from sklearn.ensemble import ExtraTreesClassifier
model = ExtraTreesClassifier()
model.fit(X,y)

feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(20).plot(kind='barh')
plt.show()


# ## Kmeans and tree method gives feature importance differently

features_tree_based = list(feat_importances.sort_values(ascending = False).head(20).index)

features_tree_based

list(featureScores.nlargest(20, 'Score')['Specs'])

# !jupytext --to py Feature_Selection.ipynb


